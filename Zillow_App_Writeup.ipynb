{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b41e61b-4315-4e9a-83b7-f74ac8c0f19d",
   "metadata": {},
   "source": [
    "# Is this Zillow listing overpriced?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5fbbb6-a099-4055-98d3-f7bf936cda25",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6600494-2a3e-4c22-859b-abbbcae36d87",
   "metadata": {},
   "source": [
    "The goal of this project was to build out a resilient web scrapping and data storage pipeline for homes on zillow and present a comparative analysis of Zilow homes on a per town basis through a Streamlit app.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46e634-ecac-4514-8057-bbbc1aa1f00c",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0afec16-801d-413b-871a-9cbc5ceba953",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "The flow of my application is as follows: my Streamlit app accepts a home link as input. It webscrappes this specific home using BeautifulSoup, and establishes if any important fields are missing. It then pulls the \"town/city\" designation for this home, and checks a pickled dictionary of timestamps for the last time it webscrapped this town for new sales. If it was scraped within the last week, it runs a tailored regression on the fields scrapped from the input home, on the data that has been saved. If it hasn't been scrapped in the last week, it regenerates the list of new sales from that town, and scrapes any that have not been previously been scrapped, and runs the tailored regression on the updated sales (saving the new homes, and updating the timestamp in the process). \n",
    "\n",
    " ![title](Application_Flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa83b5-8c61-4c7e-8628-79e3c6484223",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49306808-7bcb-4560-81eb-1552800a0881",
   "metadata": {},
   "source": [
    "The challenge of this project was building a tool that could handle all of the variable inputs that come with a Zillow link. Some homes are missing squre feet, others are missing zestimates, etc. Dropping all homes with nulls can help clean data, but it does not solve the problem that the input home itself could be missing important fields (we can't drop the input home!) Another challenge is webscrapping speed. Having designed a scrapping and regression method that can accept homes from anywhere, new scrapes are done at just <1 sec a home. In a dataset of a few homes, that is far too long. My solution to this was to create a timesetamp dictionary for stored scrapes. \n",
    "\n",
    "My final scrapped dataset fields (assuming all fields exist for the input home) are:\n",
    "- Price\n",
    "- Beds\n",
    "- Baths\n",
    "- Square footage\n",
    "- Address (lattitude, longitude, street address, city, state, zip)\n",
    "- Year Built\n",
    "- Lot Size\n",
    "- Days on Zillow\n",
    "- Zestimate\n",
    "- Tax Assessment \n",
    "\n",
    "\n",
    "![title](Darien_home3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44dabde-5003-4174-a49a-014a21e38b1d",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ae0f3-65ac-435e-9b4d-ba595eefa9e0",
   "metadata": {},
   "source": [
    "Once my data has been stored and cleaned, I run a basic OLS regression on the data for the input town (example below). The input home is highlighted in red, with most similar homes highlighted in orange. This gives a sense for how over or under priced an input home is in relaion to its peers. Within my streamlit app, I also return what the regression predicts this home price should be, as well as what percentil this house is amongst its peers for beds, baths, square footage, and lot size.\n",
    "\n",
    "![title](OLS_Predicts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc70be0-a0d1-4581-ba7b-1471a1ecdc57",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df063215-b726-45af-a3ec-ae8c8c5c21f7",
   "metadata": {},
   "source": [
    "* BeautifulSoup for webscrapping\n",
    "* SQL, Numpy, and Pandas for data exploration\n",
    "* SQL, Pickling, and GCP for data storage \n",
    "* Streamlit for app presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15c5c8-7279-4094-a9fe-b955ca7e3e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
